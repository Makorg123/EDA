# -*- coding: utf-8 -*-
"""Deep Learning Class 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XBJKOrYxLLCW9dlE5Ob70xSVpUoDDG-5
"""

# Deep Learning - Advanced Neural Networks with many hidden layers and hundreds of neuron per layer.

# Deep Learning requires Graphical Processing Unit(GPUs) for faster processing and computation.

# Free GPU's are available at Google Colab(Python only) and kaggle(Both Python and R).

# Google Colab has both GPUs and TPUs(Tensor Processing Units).

# Tensor is a mathemetical object represented as arrays of multi dimension. Every Tensor has a Rank which is the dimension of array or vector that
# makes up the tensor.

# Libraries for Deep Learning - Tensorflow & Keras, PyTorch.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow

from keras.datasets import mnist

(X_train,y_train),(X_test,y_test) = mnist.load_data()

# Image data
print(X_train.shape)
print(X_test.shape)

# Show the image

plt.imshow(X_train[0]) # Handwritten digits images from 0 to 9.

for i in range(0,25):
  plt.subplot(5,5,i+1)
  plt.imshow(X_train[i])
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
plt.show()

# Step1: Preprocessing of Images - Reshape or convert images to pixels and pixels(numeric format of image) and pixels range from 0 to 255.

X_train = X_train.reshape(-1,784)
X_test = X_test.reshape(-1,784)

# -1 represent 1D or One Dimention
# 28 x 28 = 784

X_train[0] # this output is only for one image.

# Step 2: Preprocessing of images - Normalization or Min Max Scaler must be done to bring all values in range of 0 and 1 and positive only.

# MinMax Scaler = (X-Xmin)/(Xmax-Xmin)
# Normalization must be done for faster computation process.

X_train = X_train/255
X_test = X_test/255

X_train[0]

# Neural Network Model - Functions must be used for defining Neural Network.

# Sequential() - Model defination function. Neural Networks are sequential Linear Models.

# Dense() - to define a hidden layer. parameters to be given are number of neurons in the layer(integer), activation function to be used.
# Activation functions are linear, sigmoid, tanh, relu, softmax.

# Dropout() -  How many neurons to be dropped
# BatchNormalization() - Normalize data in batches.
# Final layer Dense() depends on the model being built. Activation function and no.of neurons depend on model being built.

# Regression Model - Activation Function is Linear (number of neurons - 1)
# Binary Classification Model - Activation Function is sigmoid(number of neurons -2).
# Multinomial classification - Activation Function is softmax(number of neurons depend on number of levels in depedent variable).

# Compile Model Metrics
# Define the optimizer to be used - adam(default), sgd, RMSprop
# Define Loss Function - Binary cross Entorpy (Binary Data), Categorical Cross Entropy(Multimnomail Data)

# Define Metrics - Classification (Accuracy & F1 Score) & Regression(RMSE & R Square.)

from keras.models import Sequential
from keras.layers import Dense

nn = Sequential()
nn.add(Dense(1024,activation="relu",input_shape=(784,))) # First hidden layer
nn.add(Dense(128,activation="relu"))
nn.add(Dense(10,activation="softmax"))

nn.compile(optimizer = "adam",loss="categorical_crossentropy",metrics=["accuracy"])

nn.summary()

# Label Encode Dependent Variable
y_train = tensorflow.keras.utils.to_categorical(y_train,10)
y_test = tensorflow.keras.utils.to_categorical(y_test,10)

# Fitting the Model
nn.fit(X_train,y_train,epochs=10,batch_size = 128, validation_data=(X_test,y_test))

"""### Prediction"""

testdata = X_train[128:200]

pred = nn.predict(testdata)

predicted_digits = np.argmax(pred,axis = 1)
print(predicted_digits)

plt.imshow(X_train[137])

